"""
ç®€å•çš„ LLM æµ‹è¯•è„šæœ¬
Simple LLM Test Script
"""
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.config import get_config
from src.llm import QwenLLMEngine


def test_llm():
    """æµ‹è¯• LLM å¼•æ“"""
    print("="*60)
    print("ğŸ¤– LLM å¼•æ“æµ‹è¯•")
    print("="*60)

    # 1. åŠ è½½é…ç½®
    print("\nğŸ“‹ åŠ è½½é…ç½®...")
    config = get_config()
    llm_config = config.get_section('llm') or {}

    api_key = llm_config.get('api_key')
    if not api_key or api_key == "sk-your-api-key-here":
        print("âŒ API Key æœªé…ç½®")
        return False

    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"   æ¨¡å‹: {llm_config.get('model', 'qwen-turbo')}")
    print(f"   API Key: {api_key[:10]}...")
    print(f"   æ¸©åº¦: {llm_config.get('temperature', 0.7)}")

    # 2. åˆå§‹åŒ–å¼•æ“
    print("\nğŸ”§ åˆå§‹åŒ– LLM å¼•æ“...")
    try:
        llm = QwenLLMEngine(
            api_key=api_key,
            model=llm_config.get('model', 'qwen-turbo'),
            temperature=llm_config.get('temperature', 0.7),
            max_tokens=llm_config.get('max_tokens', 1500),
            enable_history=llm_config.get('enable_history', True),
            max_history=llm_config.get('max_history', 10),
            system_prompt=llm_config.get('system_prompt')
        )
        print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        model_info = llm.get_model_info()
        print(f"   æ¨¡å‹: {model_info['name']}")
        print(f"   æä¾›å•†: {model_info['provider']}")
        if 'system_prompt' in model_info:
            print(f"   è§’è‰²è®¾å®š: {model_info['system_prompt']}")
    except Exception as e:
        print(f"âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # 3. æµ‹è¯•å¯¹è¯
    print("\nğŸ’¬ æµ‹è¯•å¯¹è¯ç”Ÿæˆ...")
    test_questions = [
        "ä½ å¥½",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"
    ]

    for i, question in enumerate(test_questions, 1):
        print(f"\n--- æµ‹è¯• {i} ---")
        print(f"ğŸ‘¤ ç”¨æˆ·: {question}")
        try:
            result = llm.chat(question)
            print(f"ğŸ¤– æ´¾è’™: {result['reply']}")
            print(f"   Token: {result['usage'].get('total_tokens', 0)}")
            print(f"   åŸå› : {result['finish_reason']}")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    # 4. æµ‹è¯•å¯¹è¯å†å²
    print("\nğŸ“š æµ‹è¯•å¯¹è¯å†å²...")
    print("ğŸ‘¤ ç”¨æˆ·: ä½ çŸ¥é“æˆ‘æ˜¯è°å—ï¼Ÿ")
    try:
        result = llm.chat("ä½ çŸ¥é“æˆ‘æ˜¯è°å—ï¼Ÿ")
        print(f"ğŸ¤– æ´¾è’™: {result['reply']}")
        history = llm.get_conversation_history()
        print(f"   å¯¹è¯å†å²: {len(history)} æ¡æ¶ˆæ¯")
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        return False

    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
    print("="*60)
    return True


if __name__ == '__main__':
    try:
        success = test_llm()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ä¸­æ–­")
        sys.exit(1)
